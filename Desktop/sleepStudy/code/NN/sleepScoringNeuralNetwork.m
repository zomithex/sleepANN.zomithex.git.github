function [Y,Xf,Af] = myNeuralNetworkFunction(X,~,~)
%MYNEURALNETWORKFUNCTION neural network simulation function.
%
% Generated by Neural Network Toolbox function genFunction, 13-Apr-2015 20:19:16.
% 
% [Y] = myNeuralNetworkFunction(X,~,~) takes these arguments:
% 
%   X = 1xTS cell, 1 inputs over TS timsteps
%   Each X{1,ts} = 5xQ matrix, input #1 at timestep ts.
% 
% and returns:
%   Y = 1xTS cell of 1 outputs over TS timesteps.
%   Each Y{1,ts} = 2xQ matrix, output #1 at timestep ts.
% 
% where Q is number of samples (or series) and TS is the number of timesteps.

%#ok<*RPMT0>

  % ===== NEURAL NETWORK CONSTANTS =====
  
  % Input 1
  x1_step1_xoffset = [25;16;5;0;0.038717814];
  x1_step1_gain = [0.000201106083459025;0.000823384108686702;0.000670915800067092;0.0434782608695652;0.822626440947744];
  x1_step1_ymin = -1;
  
  % Layer 1
  b1 = [-0.56222411283790175;0.33262092104648272;-0.31076252603220256;-4.8895193337617968;2.6567723415579154];
  IW1_1 = [3.5377352755827118 -1.475549781538525 -0.0042922837288085092 0.49700138320322412 4.2511932550575233;0.43855634861610976 -0.18910636284669383 1.1081980286060551 1.4423771719127261 -1.4370137532458387;2.2271899996723117 -1.8760363276998415 -0.31182362961072868 -0.99142238121728987 -1.2922261105862378;0.08711308301226639 -0.3384461510817976 -0.1974325423378655 0.22722721638091448 -7.9569764623298971;0.36707084830579145 -0.68170131615202922 -0.70175889256337809 1.0608287748588747 1.9043024280364467];
  
  % Layer 2
  b2 = [1.7356184627056455;-2.2939261019075912];
  LW2_1 = [3.6630735423561238 -1.2949978035258061 -2.6181194899824769 -8.5818366301760598 1.2996888408073877;-3.2586542394228095 0.13060562493513805 2.0145244716026292 7.5361861809187003 -2.2098174320103356];
  
  % ===== SIMULATION ========
  
  % Format Input Arguments
  isCellX = iscell(X);
  if ~isCellX, X = {X}; end;
  
  % Dimensions
  TS = size(X,2); % timesteps
  if ~isempty(X)
    Q = size(X{1},2); % samples/series
  else
    Q = 0;
  end
  
  % Allocate Outputs
  Y = cell(1,TS);
  
  % Time loop
  for ts=1:TS
  
    % Input 1
    Xp1 = mapminmax_apply(X{1,ts},x1_step1_gain,x1_step1_xoffset,x1_step1_ymin);
    
    % Layer 1
    a1 = tansig_apply(repmat(b1,1,Q) + IW1_1*Xp1);
    
    % Layer 2
    a2 = softmax_apply(repmat(b2,1,Q) + LW2_1*a1);
    
    % Output 1
    Y{1,ts} = a2;
  end
  
  % Final Delay States
  Xf = cell(1,0);
  Af = cell(2,0);
  
  % Format Output Arguments
  if ~isCellX, Y = cell2mat(Y); end
end

% ===== MODULE FUNCTIONS ========

% Map Minimum and Maximum Input Processing Function
function y = mapminmax_apply(x,settings_gain,settings_xoffset,settings_ymin)
  y = bsxfun(@minus,x,settings_xoffset);
  y = bsxfun(@times,y,settings_gain);
  y = bsxfun(@plus,y,settings_ymin);
end

% Competitive Soft Transfer Function
function a = softmax_apply(n)
  nmax = max(n,[],1);
  n = bsxfun(@minus,n,nmax);
  numer = exp(n);
  denom = sum(numer,1); 
  denom(denom == 0) = 1;
  a = bsxfun(@rdivide,numer,denom);
end

% Sigmoid Symmetric Transfer Function
function a = tansig_apply(n)
  a = 2 ./ (1 + exp(-2*n)) - 1;
end
MitraMosslemi
